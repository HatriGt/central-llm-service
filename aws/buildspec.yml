version: 0.2

phases:
  pre_build:
    commands:
      - echo "Installing dependencies..."
      - pip install huggingface_hub
      - echo "Authenticating with Hugging Face..."
      - echo "Logging into Amazon ECR..."
      - aws ecr get-login-password --region eu-central-1 | docker login --username AWS --password-stdin 396360117331.dkr.ecr.eu-central-1.amazonaws.com
  build:
    commands:
      - echo "Downloading Llama 3.1 8B Instruct model..."
      - cd docker/ecr-build
      - mkdir -p models/Llama-3.1-8B-Instruct
      - |
        python -c "
        import os
        from huggingface_hub import snapshot_download, login
        
        # Login with token
        token = os.getenv('HUGGINGFACE_HUB_TOKEN')
        if not token:
            print('‚ùå HUGGINGFACE_HUB_TOKEN not found')
            exit(1)
            
        print('üîê Authenticating with Hugging Face...')
        login(token=token)
        
        print('üöÄ Downloading Llama 3.1 8B Instruct model...')
        print('This may take a while as the model is ~16GB...')
        
        try:
            snapshot_download(
                repo_id='meta-llama/Llama-3.1-8B-Instruct',
                local_dir='models/Llama-3.1-8B-Instruct',
                token=token
            )
            print('‚úÖ Model downloaded successfully!')
        except Exception as e:
            print(f'‚ùå Error downloading model: {e}')
            exit(1)
        "
      - echo "Building Docker image..."
      - docker build -t llama31-8b-vllm .
  post_build:
    commands:
      - echo "Tagging and pushing to ECR..."
      - docker tag llama31-8b-vllm:latest 396360117331.dkr.ecr.eu-central-1.amazonaws.com/llama31-8b-vllm:latest
      - docker push 396360117331.dkr.ecr.eu-central-1.amazonaws.com/llama31-8b-vllm:latest
      - echo "‚úÖ Build completed successfully!"

artifacts:
  files:
    - '**/*'
  name: llama31-8b-vllm-$(date +%Y-%m-%d)
