{
  "family": "central-llm-service",
  "networkMode": "bridge",
  "requiresCompatibilities": ["EC2"],
  "cpu": "4096",
  "memory": "16384",
  "executionRoleArn": "arn:aws:iam::396360117331:role/ecsTaskExecutionRole",
  "taskRoleArn": "arn:aws:iam::396360117331:role/ecsTaskRole",
  "containerDefinitions": [
    {
      "name": "vllm-server",
      "image": "396360117331.dkr.ecr.eu-central-1.amazonaws.com/mistral8b-vllm:latest",
      "cpu": 4096,
      "memory": 16384,
      "essential": true,
      "entryPoint": ["/bin/sh", "-c"],
      "command": ["python3 -m vllm.entrypoints.openai.api_server --model /app/models/Ministral-8B-Instruct-2410/ --host 0.0.0.0 --port 8000 --served-model-name ministral-8b-instruct --max-model-len 24576 --gpu-memory-utilization 0.9 --trust-remote-code"],
      "portMappings": [
        {
          "containerPort": 8000,
          "hostPort": 8000,
          "protocol": "tcp"
        }
      ],
      "environment": [
        {
          "name": "CUDA_VISIBLE_DEVICES",
          "value": "0"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/central-llm-service",
          "awslogs-region": "eu-central-1",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "healthCheck": {
        "command": [
          "CMD-SHELL",
          "curl -f http://localhost:8000/health || exit 1"
        ],
        "interval": 30,
        "timeout": 5,
        "retries": 3,
        "startPeriod": 60
      },
      "resourceRequirements": [
        {
          "type": "GPU",
          "value": "1"
        }
      ]
    }
  ]
}
